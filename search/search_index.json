{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"delphi Documentation for relevant repositories to DRIFT's 2023-2024 project: LEADS. Table of Contents Project Original Repository MkDocs Documentation telemetry-stream Link Link ardupilot-logs-parsing Link Link kinematic-model Link Link PTU-SDK \ud83d\udeab Link team-drift.org \ud83d\udeab Work in progress","title":"Home"},{"location":"#delphi","text":"Documentation for relevant repositories to DRIFT's 2023-2024 project: LEADS.","title":"delphi"},{"location":"#table-of-contents","text":"Project Original Repository MkDocs Documentation telemetry-stream Link Link ardupilot-logs-parsing Link Link kinematic-model Link Link PTU-SDK \ud83d\udeab Link team-drift.org \ud83d\udeab Work in progress","title":"Table of Contents"},{"location":"PTU-SDK/","text":"","title":"PTU-SDK"},{"location":"about/","text":"Nosces inornatos victori Se roganda minimas donec inducere videntur Lorem markdownum, esses caelitibus viro. Opus nare bimari media explicat excidit usque. Tui par equique Aetne, reccidat a margine ignibus miracula numen nec talibus carmine tristisque. Victum vero meruisse arvis exegit quare cum haesit, at fronde scelus. Nostroque dicenti causas et cinxit primusque dolor Laetaris indoluit concordare cruentior inde quo iam tum quater, humi est! Fidis sed ardent, Nileus aeque carpentem et orbes. Et sorori quoque ferro manibus carchesia dedit: agmina pariente sublimis auctor. Vocisque laniger audet Romana aures Manu infans praebet cladis ferrumque cervus: gravis mea posita vero, pars niveum, terreat caelo agna mille. Est oculosque coagula memorque, modo Troades fetus si glacialis elisarum, nullis sine fumantiaque esse. Una solet et fuit est fuit pluma sic prodes marem odit Eurynomus mensis; qui. if (minimize_technology_kilobit + command(-4, ergonomics_intranet_core)) { textRwSnow = grayscaleAppFrozen + kvm(-5, 2, sdRemoteFavorites); } else { spiderLeftPop = 4; username_port(3); } var port_sd = host(box_syn.leopard_point(terahertzIos(wins), truncate_mips, noc_drm), 1, mount_function(-4)); capacityFlash = net_only(host_trackball, tcp_website, disk) + atm_io_primary; if (cachePciOs < 11) { bluSoftwarePretest *= 4; optic_infotainment(parallel_search, ribbon_rj / 2); } Amato coniunctaque victa Luctantemque procul adversae quarum at concussit ignis fraternis Cecropio est, dies imagine, non cantus admota tener! Terra est illuc sedit: et sed fui convertit ille , capillos, ut fruges, sed Saturnia Taenarides. Esse est, esse, funes aere inclinat, leones cogit inprobat domoque arbore fere. Numina quae: geminae: si dumque oritur circumspice tolles arbitrium tempora pinus. Tollit quem se tamen videt hunc curam Nec urguent ignotas perluitur patris Curvos iam vapor aequore resupino Sidus falsi venenis pressistis madida Iovis Se ultor pedibusve sumptis Pylos et manuque Pars ambae: nec quam veneni, videri increpat hominum utque, ait est Olympi gerit. Muneris propiora candida. Faciendo deus; devia notare, quae dentes socerque. Loco luctu huic etiam hausit et carmina, saepe magna nec. Nobiscum laudemque externis venatu Phoeboque, et nobis rami omen Ericthonium color attonitum pecudes in meruisse confundere vidit. Iam antrum de in, nec sanguine, iram Phoebeamque iura.","title":"Nosces inornatos victori"},{"location":"about/#nosces-inornatos-victori","text":"","title":"Nosces inornatos victori"},{"location":"about/#se-roganda-minimas-donec-inducere-videntur","text":"Lorem markdownum, esses caelitibus viro. Opus nare bimari media explicat excidit usque. Tui par equique Aetne, reccidat a margine ignibus miracula numen nec talibus carmine tristisque. Victum vero meruisse arvis exegit quare cum haesit, at fronde scelus.","title":"Se roganda minimas donec inducere videntur"},{"location":"about/#nostroque-dicenti-causas-et-cinxit-primusque-dolor","text":"Laetaris indoluit concordare cruentior inde quo iam tum quater, humi est! Fidis sed ardent, Nileus aeque carpentem et orbes. Et sorori quoque ferro manibus carchesia dedit: agmina pariente sublimis auctor.","title":"Nostroque dicenti causas et cinxit primusque dolor"},{"location":"about/#vocisque-laniger-audet-romana-aures","text":"Manu infans praebet cladis ferrumque cervus: gravis mea posita vero, pars niveum, terreat caelo agna mille. Est oculosque coagula memorque, modo Troades fetus si glacialis elisarum, nullis sine fumantiaque esse. Una solet et fuit est fuit pluma sic prodes marem odit Eurynomus mensis; qui. if (minimize_technology_kilobit + command(-4, ergonomics_intranet_core)) { textRwSnow = grayscaleAppFrozen + kvm(-5, 2, sdRemoteFavorites); } else { spiderLeftPop = 4; username_port(3); } var port_sd = host(box_syn.leopard_point(terahertzIos(wins), truncate_mips, noc_drm), 1, mount_function(-4)); capacityFlash = net_only(host_trackball, tcp_website, disk) + atm_io_primary; if (cachePciOs < 11) { bluSoftwarePretest *= 4; optic_infotainment(parallel_search, ribbon_rj / 2); }","title":"Vocisque laniger audet Romana aures"},{"location":"about/#amato-coniunctaque-victa","text":"Luctantemque procul adversae quarum at concussit ignis fraternis Cecropio est, dies imagine, non cantus admota tener! Terra est illuc sedit: et sed fui convertit ille , capillos, ut fruges, sed Saturnia Taenarides. Esse est, esse, funes aere inclinat, leones cogit inprobat domoque arbore fere. Numina quae: geminae: si dumque oritur circumspice tolles arbitrium tempora pinus. Tollit quem se tamen videt hunc curam Nec urguent ignotas perluitur patris Curvos iam vapor aequore resupino Sidus falsi venenis pressistis madida Iovis","title":"Amato coniunctaque victa"},{"location":"about/#se-ultor-pedibusve-sumptis-pylos-et-manuque","text":"Pars ambae: nec quam veneni, videri increpat hominum utque, ait est Olympi gerit. Muneris propiora candida. Faciendo deus; devia notare, quae dentes socerque. Loco luctu huic etiam hausit et carmina, saepe magna nec. Nobiscum laudemque externis venatu Phoeboque, et nobis rami omen Ericthonium color attonitum pecudes in meruisse confundere vidit. Iam antrum de in, nec sanguine, iram Phoebeamque iura.","title":"Se ultor pedibusve sumptis Pylos et manuque"},{"location":"ardupilot-logs-parsing/","text":"","title":"Ardupilot Logs Parsing"},{"location":"kinematic-model/","text":"Kinematic Model of a Moving Drone's Telemetry Stream This notebook represents a kinematic model of telemetry data from a moving drone. A kinematic model describes the movement of objects using variables such as position, velocity, and acceleration, without considering the underlying forces that cause the motion. In this case, we are predicting the next position of the drone based on its current position and velocity. Methodology Data Preprocessing The first step of the notebook is the data preprocessing, where the mavlink data of the drone and the ground control station (GCS) are loaded. We add a prefix to their packet type to be easily identified (UAV_/GCS_). The time_boot_ms column which provides the system time since boot, is normalized by the minimum time to start the timestamp from zero. All of them are then combined into one dataframe, sorted by time in ascending order, and some columns are converted into their correct units. Conversion to East, North, Up (ENU) coordinates In the field of aerial vehicles, including drones, a common coordinate system used is the East, North, Up (ENU) system. In this system: The x-axis points towards the North. The y-axis points towards the East. The z-axis points Upwards. To transform the GPS (latitude, longitude, altitude) coordinates into ENU, we use the geodetic2enu function from the pymap3d library. The conversion needs a reference point, which is in this case the starting point of the drone. \u26a0\ufe0f E, N, U is not in the same order as X, Y, Z. The result of pymap3d.geodetic2enu() must be unpacked like y, x, z = pymap3d.geodetic2enu() Kinematic Prediction To predict the position of the drone in the next timestamp, we use a basic kinematic equation: position_next = position_current + velocity_current * time_to_next Where time_to_next is the difference in time between two consecutive timestamps. This equation assumes that the drone moves at a constant velocity, thus it's a simple approximation that does not consider any changes in velocity (acceleration). In this example, we are forecasting the drone's location for the upcoming timestamp. This allows us to compare our prediction with the actual reported location at that subsequent timestamp. Evaluation The predictions are evaluated by calculating the Euclidean distance between the predicted and actual positions. The Euclidean distance between two points in 3D space is calculated as follows: distance = sqrt((x1-x2)^2 + (y1-y2)^2 + (z1-z2)^2) The distances are then converted into centimeters and visualized by a histogram. What these distances mean The distances calculated in this model represent the adjustments anticipated in real-time drone operations. Specifically, the gimbal on the drone will follow each predicted vector until it reaches the predicted location. Then it will need to \"jump\" to the next actual reported location. These 'jumps' are represented by the distances we have calculated. Assuming that the reported locations are completely accurate, these distances provide a good estimation of the potential error expected in the model. They could be used to inform the design parameters of the receiving device on the drone. However, it's important to consider some degree of error in the reported location data itself. Additionally, the mechanical inaccuracies or latency in the drone's pan-tilt unit can also contribute to the overall model error. Therefore, due to the combined implications of location data inaccuracies and the mechanical response of the drone's pan-tilt unit, the calculated error should not be solely used to determine the design requirements of the receiving device on the drone. Visualization Although drone motion in 3D space is best visualized with a 3D plot, 2D plots might be more convenient in some situations. Thus, both 3D and 2D plots are made, where each point corresponds to a timestamp, and a line is drawn to the point that represents the predicted position for the next timestamp. Fourier Transform Analysis I used the Fourier Transform as a tool to analyze the inconsistency in the vertical velocity ('vz') of the drone. Initially, the error in the z-direction of the drone movement wasn't making sense, so I used the spectral analysis to examine the dominant frequencies that make up the 'vz' signal. On plotting these frequencies, I noticed a significant spike at approximately 1.2Hz. Interestingly, this dominant frequency at 1.2Hz matched the frequency observed from my smartphone's accelerometer while I was walking with the drone. In essence, the Fourier Transform helped me realize that the drone's vertical velocity was significantly influenced by my walking rhythm.","title":"Kinematic Model"},{"location":"kinematic-model/#kinematic-model-of-a-moving-drones-telemetry-stream","text":"This notebook represents a kinematic model of telemetry data from a moving drone. A kinematic model describes the movement of objects using variables such as position, velocity, and acceleration, without considering the underlying forces that cause the motion. In this case, we are predicting the next position of the drone based on its current position and velocity.","title":"Kinematic Model of a Moving Drone's Telemetry Stream"},{"location":"kinematic-model/#methodology","text":"","title":"Methodology"},{"location":"kinematic-model/#data-preprocessing","text":"The first step of the notebook is the data preprocessing, where the mavlink data of the drone and the ground control station (GCS) are loaded. We add a prefix to their packet type to be easily identified (UAV_/GCS_). The time_boot_ms column which provides the system time since boot, is normalized by the minimum time to start the timestamp from zero. All of them are then combined into one dataframe, sorted by time in ascending order, and some columns are converted into their correct units.","title":"Data Preprocessing"},{"location":"kinematic-model/#conversion-to-east-north-up-enu-coordinates","text":"In the field of aerial vehicles, including drones, a common coordinate system used is the East, North, Up (ENU) system. In this system: The x-axis points towards the North. The y-axis points towards the East. The z-axis points Upwards. To transform the GPS (latitude, longitude, altitude) coordinates into ENU, we use the geodetic2enu function from the pymap3d library. The conversion needs a reference point, which is in this case the starting point of the drone. \u26a0\ufe0f E, N, U is not in the same order as X, Y, Z. The result of pymap3d.geodetic2enu() must be unpacked like y, x, z = pymap3d.geodetic2enu()","title":"Conversion to East, North, Up (ENU) coordinates"},{"location":"kinematic-model/#kinematic-prediction","text":"To predict the position of the drone in the next timestamp, we use a basic kinematic equation: position_next = position_current + velocity_current * time_to_next Where time_to_next is the difference in time between two consecutive timestamps. This equation assumes that the drone moves at a constant velocity, thus it's a simple approximation that does not consider any changes in velocity (acceleration). In this example, we are forecasting the drone's location for the upcoming timestamp. This allows us to compare our prediction with the actual reported location at that subsequent timestamp.","title":"Kinematic Prediction"},{"location":"kinematic-model/#evaluation","text":"The predictions are evaluated by calculating the Euclidean distance between the predicted and actual positions. The Euclidean distance between two points in 3D space is calculated as follows: distance = sqrt((x1-x2)^2 + (y1-y2)^2 + (z1-z2)^2) The distances are then converted into centimeters and visualized by a histogram.","title":"Evaluation"},{"location":"kinematic-model/#what-these-distances-mean","text":"The distances calculated in this model represent the adjustments anticipated in real-time drone operations. Specifically, the gimbal on the drone will follow each predicted vector until it reaches the predicted location. Then it will need to \"jump\" to the next actual reported location. These 'jumps' are represented by the distances we have calculated. Assuming that the reported locations are completely accurate, these distances provide a good estimation of the potential error expected in the model. They could be used to inform the design parameters of the receiving device on the drone. However, it's important to consider some degree of error in the reported location data itself. Additionally, the mechanical inaccuracies or latency in the drone's pan-tilt unit can also contribute to the overall model error. Therefore, due to the combined implications of location data inaccuracies and the mechanical response of the drone's pan-tilt unit, the calculated error should not be solely used to determine the design requirements of the receiving device on the drone.","title":"What these distances mean"},{"location":"kinematic-model/#visualization","text":"Although drone motion in 3D space is best visualized with a 3D plot, 2D plots might be more convenient in some situations. Thus, both 3D and 2D plots are made, where each point corresponds to a timestamp, and a line is drawn to the point that represents the predicted position for the next timestamp.","title":"Visualization"},{"location":"kinematic-model/#fourier-transform-analysis","text":"I used the Fourier Transform as a tool to analyze the inconsistency in the vertical velocity ('vz') of the drone. Initially, the error in the z-direction of the drone movement wasn't making sense, so I used the spectral analysis to examine the dominant frequencies that make up the 'vz' signal. On plotting these frequencies, I noticed a significant spike at approximately 1.2Hz. Interestingly, this dominant frequency at 1.2Hz matched the frequency observed from my smartphone's accelerometer while I was walking with the drone. In essence, the Fourier Transform helped me realize that the drone's vertical velocity was significantly influenced by my walking rhythm.","title":"Fourier Transform Analysis"},{"location":"telemetry-stream/","text":"Telemetry Stream telemetry-stream is responsible for streaming telemetry data from the drone we are tracking to our kinematic-model which will then perform the necessary calculations needed by PTU-SDK to move the gimbal controlling the laser. In this documentation, we (will) outline what is inside telemetry-stream , how telemetry-stream works, and more importantly how to use it. telemetry_stream.cpp As of 03/13/2024, telemetry_stream.cpp is functional. We are currently working on streaming the specific fields needed by DAM . For now, we are streaming altitude only. This is a C++ implementation for streaming telemetry data. We primarily utilize MAVSDK , a higher level API \"that aims to be fully standards-compliant with MAVLink common microservices\" such as telemetry, hence why we opted for it. There is no reason not to re-use already fast (C++) code that's already been written for this \"basic\" functionality (unless MAVSDK loses the wide community support it has as of today). deprecated The deprecated folder contains a python implementation for telemetry streaming which heavily utilizes pymavlink . Though still functional, we opted to refactor in C++ as performance must be optimized. Prior Knowledge Read the following pieces of documentation (in a BFS manner) before developing: MAVLink : read the Introduction section and its subsections MAVSDK : read the Introduction and C++ sections, but do not start the Quickstart section yet Telemetry Guide Telemetry Class Discrepancies : always watch out for discrepancies between the actual documentation and example code provided by a given tool. However, you should always reference both. MAVSDK Setup To begin developing, read the notes below as you follow the whole C++ Quickstart for your machine's OS. After the Install MAVSDK library section, check its version. For example, if you're on macOS, then brew info mavsdk . You may run into a versioning discrepancy issue between the mavsdk library installed during the Install MAVSDK Library section and the mavsdk repository in the Build and Try Example section. If the versions between the library and the repository differ, simply pull the whole version of the repo that matches the library, or alternatively copy and paste the takeoff_and_land example code from the version of the repo that matches the library. To switch to the appropiate version of the MAVSDK repo: 1. go to the MAVSDK repository 2. Click on \"main\" -> \"Tags\" 3. Select the appropriate version Once you reach the Setting up a Simulator subsection, refer to \"Test Simulation\" within this documentation page. Test Simulation Download QGroundControl and simply open it. Download PX-4 and build the simulation. Open a terminal. Run the following: git clone https://github.com/PX4/PX4-Autopilot.git --recursive cd PX4-Autopilot make px4_sitl jmavsim Ensure Proper Configurations Ensure QGroundControl ports and \\<insert testing file from MAVSDK repo such as takeoff_and_land.cpp > ports are set to 14540 (Or, whatever port we want. Just have it be uniform.) Go to the top left of QGroundCountrol -> Application Settings -> MAVLink, and check if the hostname is localhost:14445 Start up the simulation Simply follow the rest of the instructions outlined here . Additional Steps for ARM Architecture/Apple Silicon Download Anaconda Create environment for x86_64 architecture CONDA_SUBDIR=osx-64 conda create -n px4_x86 python=3.9 Activate conda activate px4_x86 Set Environment conda config --env --set subdir osx-64 ** JMavSim's GUI does not work within ARM, to run the simulation without a gui, HEADLESS=1 make px4_sitl jmavsim How to use telemetry_stream.cpp This section (will soon) outlines how to physically setup telemetry streaming from the drone to the ground station (us). Ensure you have completed ALL sections above beforehand. Installations Run the command: brew install nlohmann-json You can use the deprecrated/stream_data.py as a quick validation that your baud and connection_string is correct. Simply plug in the aformentioned parameters into stream_data.py then run the file. Otherwise, you can follow the steps from \"Test Simulation\" to setup a simulated drone rather than a physical drone to verify telemetry data streaming is working. Run telemetry_stream.cpp Build the executable cmake -Bbuild -H. cmake --build build -j8 Run the executable build/telemetry_stream udp://:14540 Run the main.cpp start up px4 jmav in this repo run the command: ./telemetry-stream you can run commander takeoff commander land this run simulation.py in agogos Python Bindings Excecute CMake file in the root directory with cmake . Within /python_bindings, the module should be ready as a .so (shared object file) Within your Python File, you should know import _telemetry_stream The Python File should be in the same directory as the .so file","title":"Telemetry Stream"},{"location":"telemetry-stream/#telemetry-stream","text":"telemetry-stream is responsible for streaming telemetry data from the drone we are tracking to our kinematic-model which will then perform the necessary calculations needed by PTU-SDK to move the gimbal controlling the laser. In this documentation, we (will) outline what is inside telemetry-stream , how telemetry-stream works, and more importantly how to use it.","title":"Telemetry Stream"},{"location":"telemetry-stream/#telemetry_streamcpp","text":"As of 03/13/2024, telemetry_stream.cpp is functional. We are currently working on streaming the specific fields needed by DAM . For now, we are streaming altitude only. This is a C++ implementation for streaming telemetry data. We primarily utilize MAVSDK , a higher level API \"that aims to be fully standards-compliant with MAVLink common microservices\" such as telemetry, hence why we opted for it. There is no reason not to re-use already fast (C++) code that's already been written for this \"basic\" functionality (unless MAVSDK loses the wide community support it has as of today).","title":"telemetry_stream.cpp"},{"location":"telemetry-stream/#deprecated","text":"The deprecated folder contains a python implementation for telemetry streaming which heavily utilizes pymavlink . Though still functional, we opted to refactor in C++ as performance must be optimized.","title":"deprecated"},{"location":"telemetry-stream/#prior-knowledge","text":"Read the following pieces of documentation (in a BFS manner) before developing: MAVLink : read the Introduction section and its subsections MAVSDK : read the Introduction and C++ sections, but do not start the Quickstart section yet Telemetry Guide Telemetry Class Discrepancies : always watch out for discrepancies between the actual documentation and example code provided by a given tool. However, you should always reference both.","title":"Prior Knowledge"},{"location":"telemetry-stream/#mavsdk-setup","text":"To begin developing, read the notes below as you follow the whole C++ Quickstart for your machine's OS. After the Install MAVSDK library section, check its version. For example, if you're on macOS, then brew info mavsdk . You may run into a versioning discrepancy issue between the mavsdk library installed during the Install MAVSDK Library section and the mavsdk repository in the Build and Try Example section. If the versions between the library and the repository differ, simply pull the whole version of the repo that matches the library, or alternatively copy and paste the takeoff_and_land example code from the version of the repo that matches the library. To switch to the appropiate version of the MAVSDK repo: 1. go to the MAVSDK repository 2. Click on \"main\" -> \"Tags\" 3. Select the appropriate version Once you reach the Setting up a Simulator subsection, refer to \"Test Simulation\" within this documentation page.","title":"MAVSDK Setup"},{"location":"telemetry-stream/#test-simulation","text":"Download QGroundControl and simply open it. Download PX-4 and build the simulation. Open a terminal. Run the following: git clone https://github.com/PX4/PX4-Autopilot.git --recursive cd PX4-Autopilot make px4_sitl jmavsim Ensure Proper Configurations Ensure QGroundControl ports and \\<insert testing file from MAVSDK repo such as takeoff_and_land.cpp > ports are set to 14540 (Or, whatever port we want. Just have it be uniform.) Go to the top left of QGroundCountrol -> Application Settings -> MAVLink, and check if the hostname is localhost:14445 Start up the simulation Simply follow the rest of the instructions outlined here .","title":"Test Simulation"},{"location":"telemetry-stream/#additional-steps-for-arm-architectureapple-silicon","text":"Download Anaconda Create environment for x86_64 architecture CONDA_SUBDIR=osx-64 conda create -n px4_x86 python=3.9 Activate conda activate px4_x86 Set Environment conda config --env --set subdir osx-64 ** JMavSim's GUI does not work within ARM, to run the simulation without a gui, HEADLESS=1 make px4_sitl jmavsim","title":"Additional Steps for ARM Architecture/Apple Silicon"},{"location":"telemetry-stream/#how-to-use-telemetry_streamcpp","text":"This section (will soon) outlines how to physically setup telemetry streaming from the drone to the ground station (us). Ensure you have completed ALL sections above beforehand.","title":"How to use telemetry_stream.cpp"},{"location":"telemetry-stream/#installations","text":"Run the command: brew install nlohmann-json You can use the deprecrated/stream_data.py as a quick validation that your baud and connection_string is correct. Simply plug in the aformentioned parameters into stream_data.py then run the file. Otherwise, you can follow the steps from \"Test Simulation\" to setup a simulated drone rather than a physical drone to verify telemetry data streaming is working.","title":"Installations"},{"location":"telemetry-stream/#run-telemetry_streamcpp","text":"Build the executable cmake -Bbuild -H. cmake --build build -j8 Run the executable build/telemetry_stream udp://:14540","title":"Run telemetry_stream.cpp"},{"location":"telemetry-stream/#run-the-maincpp","text":"start up px4 jmav in this repo run the command: ./telemetry-stream you can run commander takeoff commander land this run simulation.py in agogos","title":"Run the main.cpp"},{"location":"telemetry-stream/#python-bindings","text":"Excecute CMake file in the root directory with cmake . Within /python_bindings, the module should be ready as a .so (shared object file) Within your Python File, you should know import _telemetry_stream The Python File should be in the same directory as the .so file","title":"Python Bindings"}]}